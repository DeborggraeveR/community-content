---
path: "/tutorials/install-kubernetes-ha-cluster"
slug: "tutorial-template"
date: "2019-09-25"
title: "Install kubernetes HA cluster"
short_description: "How to install a high available kubernetes cluster on Hetzner Cloud."
tags: ["Hetzner Cloud", "Kubernetes", "HA"]
author: "Deborggraeve Randy"
author_link: "https://github.com/DeborggraeveR"
author_img: "https://avatars2.githubusercontent.com/u/11926967"
author_description: ""
language: "en"
available_languages: ["en"]
header_img: ""
---

## Introduction

In this tutorial we will create a high availability kubernetes cluster with 3 master nodes, 2 worker nodes and a HA-Proxy server.
All servers are running Ubuntu 18.04 and are added to a private network (10.0.0.0/24)
It is adviced to create a non root user on each server with root priveleges.

As container runtime we will be using CRIO
As network plugin we will be using calico

**Prerequisites**

6 or more Hetzner cloud servers.

k8s-gateway:
* Type: CX11
* IPv4: 10.0.0.200

k8s-master-001:
* Type: CX21
* IPv4: 10.0.0.2

k8s-master-002:
* Type: CX21
* IPv4: 10.0.0.3

k8s-master-003:
* Type: CX21
* IPv4: 10.0.0.4

k8s-worker-001:
* Type: CX21
* IPv4: 10.0.0.5

k8s-worker-002:
* Type: CX21
* IPv4: 10.0.0.6

## Step 1 - Setup the HA-Proxy

The HA-Proxy server is responsible for the load balancing of the master servers, to make sure at least one of the master server is reachable by the cluster.

**Install HA-Proxy**

`apt-get install haproxy`

**Update the ha-proxy config file**

`nano /etc/haproxy/haproxy.cfg`

```
defaults
        ...
        option forwardfor
        option http-server-close
        ...

frontend http_stats
        bind *:8080
        mode http
        stats uri /haproxy?stats

frontend kubernetes-api
        bind *:6443
        mode tcp
        option tcplog
        timeout client 10800s
        default_backend kubernetes-api

backend kubernetes-api
        mode tcp
        option tcplog
        option tcp-check
        balance leastconn
        timeout server 10800s
        server k8s-master-001 10.0.0.2:6443 check fall 3 rise 2
        server k8s-master-002 10.0.0.3:6443 check fall 3 rise 2
        server k8s-master-003 10.0.0.4:6443 check fall 3 rise 2
```

**Restart HA-Proxy**

`systemctl restart haproxy`

## Step 2 - Prepare all nodes

These steps must executed on all nodes

### Step 2.1 - Setup CRIO module and network requirements

```
cat > /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF
```

```
cat > /etc/sysctl.d/90-kubernetes-cri.conf <<EOF
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
```

`sysctl --system`

`reboot`

### Step 2.2 - Install CRIO

`apt-get install software-properties-common -y`

`add-apt-repository ppa:projectatomic/ppa`

`apt-get install cri-o-1.15 -y`

### Step 2.3 - Enable docker.io repository

`nano /etc/crio/crio.conf`

Uncomment the *registries* so we can pull images from docker.io or any other repository.
```
registries = [
        "docker.io",
]
```

### Step 2.4 - Start CRIO

`systemctl enable crio`

`systemctl start crio`

### Step 2.5 - Install kubernetes

`apt-get install apt-transport-https curl -y`

`curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add`

```
cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
```

`apt update`

`apt install kubelet kubeadm kubectl -y`

`apt-mark hold kubelet kubeadm kubectl`

Add some additional parameters to the kubelet runtime by editing the default kubelet file.
Make sure u set the correct node_ip on each server!

`nano /etc/default/kubelet`

```
KUBELET_EXTRA_ARGS=--feature-gates="AllAlpha=false,RunAsGroup=true" --container-runtime=remote --cgroup-driver=systemd --container-runtime-endpoint='unix:///var/run/crio/crio.sock' --runtime-request-timeout=5m --node-ip=<node_ip>
```

## Step 3 - Setup k8s-master-001

### Step 3.1 - Initialize the cluster

**Create the initial kubeadm config file**

Make sure to set the correct certSANs domain.
This is a dns entry pointing to the HA-Proxy public ip. This allows to run the kubectl from the local pc.

`sudo nano kubeadm-config.yaml`

```yaml
apiVersion: kubeadm.k8s.io/v1beta2
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: "10.0.0.2"
  bindPort: 6443
---
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: stable
clusterName: "hetzner-nuremberg"
controlPlaneEndpoint: "10.0.0.200:6443"
apiServer:
  certSANs:
  - "k8s.domain.example"
networking:
  serviceSubnet: "10.96.0.0/12"
  podSubnet: "192.168.0.0/16"
  dnsDomain: "cluster.hetzner-nuremberg"
```

`sudo kubeadm init --config=kubeadm-config.yaml --upload-certs`

`mkdir -p $HOME/.kube`

`sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config`

`sudo chown $(id -u):$(id -g) $HOME/.kube/config`

### Step 3.2 - Copy the cluster join information

Make sure u copy the kubeadm join info from the init output. We need this later.

Note that we have two kubeadm join commands in the output, one for the additional masters and one for the worker nodes.

### Step 3.3 - Install the network plugin

`curl -O https://docs.projectcalico.org/v3.9/manifests/calico.yaml`

`nano calico.yaml`

> Change veth_mtu from 1440 to 1430 (has to be 20 less as the vlan mtu)

`kubectl apply -f calico.yaml`

### Step 3.4 - Wait for all pods to start

Initially the pods that run on the pod network will use ip range 10.88.0.0, this will be fixed once we reboot the servers after the full cluster initialization.

`watch -n 1 kubectl get pods -o wide -n kube-system`

## Step 4 - Join k8s-master-002

To join the cluster, copy the kubeadm join information from step 3.2 and add the correct api advertis address to the command.

```
sudo kubeadm join 10.0.0.200:6443 --token <token> \
    --discovery-token-ca-cert-hash <cert-hash> \
    --control-plane \
    --certificate-key <cert-key> \
    --apiserver-advertise-address 10.0.0.3
```

Again, wait for all pods to get up and running

## Step 5 - Join k8s-master-003

To join the cluster, copy the kubeadm join information from step 3.2 and add the correct api advertis address to the command.

```
sudo kubeadm join 10.0.0.200:6443 --token <token> \
    --discovery-token-ca-cert-hash <cert-hash> \
    --control-plane \
    --certificate-key <cert-key> \
    --apiserver-advertise-address 10.0.0.4
```

Again, wait for all pods to get up and running

## Step 6 - Join the worker nodes

On each of the worker nodes, execute the kubeadm join command from step 3.2

```
kubeadm join 10.0.0.2:6443 --token <token> \
    --discovery-token-ca-cert-hash <cert-hash>
```

## Step 7 - Reboot all nodes

As some of the pods running on the pod subnet have an incorrect ip address, it's good to reboot all servers.
This will also reschedule the dns services on the worker nodes.

## Conclusion

Now u have a high availible kubernetes cluster.
More worker nodes can be added as needed.

## Security

To secure the cluster, i replaced the k8s-gateway node, with a pfsense server.
See: https://community.hetzner.com/tutorials/how-to-route-cloudserver-over-private-network-using-pfsense-and-hcnetworks
This allows to run the entire cluster on the private network behind a firewall. pfsense also support load balancing for the master nodes.

## Known issues

All pods using the host IP, use the public ip instead of the internal ip. This causes no issues with running the cluster.
Again it's adviced to run the cluster in a full private network without public ip assigned. (See security)

Currently there is a known bug in Ubuntu 18.04, where the local dns stop working once u run pods that use a hostPort.
This can be fixed by relinking the /etc/resolve.conf
See: https://github.com/kubernetes/kubernetes/issues/66067
